import{_ as s,c as a,d as e,o as i}from"./app-DorCUO6C.js";const l={};function t(p,n){return i(),a("div",null,n[0]||(n[0]=[e(`<h1 id="using-inspireface-in-cuda" tabindex="-1"><a class="header-anchor" href="#using-inspireface-in-cuda"><span>Using InspireFace in CUDA</span></a></h1><p>The GPU version of InspireFace has been verified on Linux physical machines. The dependency versions used for project compilation, unit testing, and continuous integration are as follows:</p><ul><li><strong>System</strong>: Ubuntu 22.04</li><li><strong>CUDA</strong>: 12.2</li><li><strong>cuDNN</strong>: 8.9.2</li><li><strong>TensorRT</strong>: 10.8.0.43</li></ul><p>The above versions were only tested on my RTX3060 12G, but this doesn&#39;t mean these are the only supported configurations. I believe as long as your environment supports TensorRT-10 and above (as of March 2025), it should run properly, since TensorRT-10 is a relatively new version, and its API seems to have some differences compared to previous versions.</p><h2 id="checking-cuda-installation" tabindex="-1"><a class="header-anchor" href="#checking-cuda-installation"><span>Checking CUDA Installation</span></a></h2><p>When using the GPU version of InspireFace, you may need to ensure your device has certain dependencies installed. Besides basic tools like GCC, CMake, Git, etc., you should mainly focus on GPU-related environments:</p><ol><li>Check CUDA installation:</li></ol><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code><span class="line">nvcc <span class="token parameter variable">-V</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>If the output looks like this, then it&#39;s OK:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code><span class="line">nvcc: NVIDIA <span class="token punctuation">(</span>R<span class="token punctuation">)</span> Cuda compiler driver</span>
<span class="line">Copyright <span class="token punctuation">(</span>c<span class="token punctuation">)</span> <span class="token number">2005</span>-2023 NVIDIA Corporation</span>
<span class="line">Built on Tue_Aug_15_22:02:13_PDT_2023</span>
<span class="line">Cuda compilation tools, release <span class="token number">12.2</span>, V12.2.140</span>
<span class="line">Build cuda_12.2.r12.2/compiler.33191640_0</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ol start="2"><li>Check cuDNN installation:</li></ol><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line">cat /usr/include/x86_64-linux-gnu/cudnn_version_v8.h | grep CUDNN_MAJOR -A 2</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>If the output looks like this, then it&#39;s OK:</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line">#define CUDNN_MAJOR 8</span>
<span class="line">#define CUDNN_MINOR 9</span>
<span class="line">#define CUDNN_PATCHLEVEL 2</span>
<span class="line">--</span>
<span class="line">#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)</span>
<span class="line"></span>
<span class="line">/* cannot use constexpr here since this is a C-only file */</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ol start="3"><li>Check your NVIDIA driver:</li></ol><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line">nvidia-smi</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>After completing these checks, you can proceed to the next step. If there are issues, you&#39;ll need to reconfigure CUDA to ensure it can run.</p><h2 id="method-1-local-compilation" tabindex="-1"><a class="header-anchor" href="#method-1-local-compilation"><span>Method 1: Local Compilation</span></a></h2><p>Local compilation is the most complex method, but if you need to modify the InspireFace source code, this method is the most suitable for you.</p><h3 id="installing-tensorrt" tabindex="-1"><a class="header-anchor" href="#installing-tensorrt"><span>Installing TensorRT</span></a></h3><p>We recommend using TensorRT-10.8 version, as it has been verified:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code><span class="line"><span class="token comment"># Download</span></span>
<span class="line"><span class="token function">wget</span> https://developer.nvidia.com/downloads/compute/machine-learning/tensorrt/10.8.0/tars/TensorRT-10.8.0.43.Linux.x86_64-gnu.cuda-12.8.tar.gz</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>After downloading, you need to extract it and place it anywhere. You can use the environment variable <strong>TENSORRT_ROOT</strong> to record its location.</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line">export TENSORRT_ROOT=/home/tunm/software/TensorRT-10.8.0.43/</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>To verify if the <strong>TensorRT</strong> installation is successful or available, you can execute the bin provided in the TensorRT folder or use some methods provided by the official website.</p><h3 id="executing-compilation" tabindex="-1"><a class="header-anchor" href="#executing-compilation"><span>Executing Compilation</span></a></h3><p>Enter the InspireFace directory, and you can directly execute the compilation command. During compilation, you need to enable some CUDA-supporting compilation switches:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code><span class="line"><span class="token comment"># Set tensorrt root dir</span></span>
<span class="line"><span class="token builtin class-name">export</span> <span class="token assign-left variable">TENSORRT_ROOT</span><span class="token operator">=</span>/home/tunm/software/TensorRT-10.8.0.43/</span>
<span class="line"><span class="token comment"># Execute build</span></span>
<span class="line">cmake <span class="token punctuation">..</span> <span class="token parameter variable">-DTENSORRT_ROOT</span><span class="token operator">=</span><span class="token variable">$TENSORRT_ROOT</span> <span class="token parameter variable">-DISF_ENABLE_TENSORRT</span><span class="token operator">=</span>ON</span>
<span class="line"><span class="token function">make</span> <span class="token parameter variable">-j8</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>If you encounter compilation-related errors at this step, it&#39;s likely that there&#39;s an issue when linking CUDA or TensorRT. Common problems include:</p><ul><li>Cannot find CUDA library</li><li>Cannot find TensorRT library</li><li>Incomplete CUDA library linking</li><li>CUDA library environment variable issues</li></ul><p>You can check the <strong>toolchain/FindTensorRT.cmake</strong> file in the directory, which contains the toolchain methods for how the project searches for GPU dependency libraries:</p><div class="language-cmake line-numbers-mode" data-highlighter="prismjs" data-ext="cmake"><pre><code><span class="line"><span class="token comment"># FindTensorRT.cmake - Simple Version</span></span>
<span class="line"><span class="token comment"># Contains basic functionality for finding TensorRT libraries</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># Find CUDA</span></span>
<span class="line"><span class="token keyword">find_package</span><span class="token punctuation">(</span>CUDA REQUIRED<span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">include_directories</span><span class="token punctuation">(</span><span class="token punctuation">\${</span>CUDA_INCLUDE_DIRS<span class="token punctuation">}</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">link_directories</span><span class="token punctuation">(</span><span class="token punctuation">\${</span>CUDA_TOOLKIT_ROOT_DIR<span class="token punctuation">}</span>/lib64<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># Find TensorRT header files</span></span>
<span class="line"><span class="token keyword">find_path</span><span class="token punctuation">(</span>TENSORRT_INCLUDE_DIR NvInfer.h</span>
<span class="line">    HINTS <span class="token punctuation">\${</span><span class="token variable">TENSORRT_ROOT</span><span class="token punctuation">}</span></span>
<span class="line">    PATH_SUFFIXES include<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># Find TensorRT libraries</span></span>
<span class="line"><span class="token keyword">find_library</span><span class="token punctuation">(</span>TENSORRT_LIBRARY_INFER nvinfer</span>
<span class="line">    HINTS <span class="token punctuation">\${</span><span class="token variable">TENSORRT_ROOT</span><span class="token punctuation">}</span></span>
<span class="line">    PATH_SUFFIXES lib lib64<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token keyword">find_library</span><span class="token punctuation">(</span>TENSORRT_LIBRARY_RUNTIME nvinfer_plugin</span>
<span class="line">    HINTS <span class="token punctuation">\${</span><span class="token variable">TENSORRT_ROOT</span><span class="token punctuation">}</span></span>
<span class="line">    PATH_SUFFIXES lib lib64<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># Find CUDA runtime library</span></span>
<span class="line"><span class="token keyword">find_library</span><span class="token punctuation">(</span>CUDA_RUNTIME_LIBRARY cudart</span>
<span class="line">    HINTS <span class="token punctuation">\${</span>CUDA_TOOLKIT_ROOT_DIR<span class="token punctuation">}</span></span>
<span class="line">    PATH_SUFFIXES lib64 lib lib64/stubs lib/stubs<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># Set result variables, can be used in projects that include this module</span></span>
<span class="line"><span class="token keyword">set</span><span class="token punctuation">(</span>ISF_TENSORRT_INCLUDE_DIRS <span class="token punctuation">\${</span>TENSORRT_INCLUDE_DIR<span class="token punctuation">}</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">set</span><span class="token punctuation">(</span>ISF_TENSORRT_LIBRARIES <span class="token punctuation">\${</span>TENSORRT_LIBRARY_INFER<span class="token punctuation">}</span> <span class="token punctuation">\${</span>TENSORRT_LIBRARY_RUNTIME<span class="token punctuation">}</span> <span class="token punctuation">\${</span>CUDA_RUNTIME_LIBRARY<span class="token punctuation">}</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># Output status messages</span></span>
<span class="line"><span class="token keyword">message</span><span class="token punctuation">(</span>STATUS <span class="token string">&quot;Found TensorRT include: <span class="token interpolation"><span class="token punctuation">\${</span><span class="token variable">TENSORRT_INCLUDE_DIR</span><span class="token punctuation">}</span></span>&quot;</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">message</span><span class="token punctuation">(</span>STATUS <span class="token string">&quot;Found TensorRT libraries: <span class="token interpolation"><span class="token punctuation">\${</span><span class="token variable">TENSORRT_LIBRARY_INFER</span><span class="token punctuation">}</span></span> <span class="token interpolation"><span class="token punctuation">\${</span><span class="token variable">TENSORRT_LIBRARY_RUNTIME</span><span class="token punctuation">}</span></span>&quot;</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">message</span><span class="token punctuation">(</span>STATUS <span class="token string">&quot;Found CUDA runtime library: <span class="token interpolation"><span class="token punctuation">\${</span><span class="token variable">CUDA_RUNTIME_LIBRARY</span><span class="token punctuation">}</span></span>&quot;</span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="compilation-completion" tabindex="-1"><a class="header-anchor" href="#compilation-completion"><span>Compilation Completion</span></a></h3><p>When you successfully compile, you can see directories like lib, test, and sample, and find dynamic libraries, test programs, and sample programs from them.</p><h2 id="method-2-using-docker-for-compilation" tabindex="-1"><a class="header-anchor" href="#method-2-using-docker-for-compilation"><span>Method 2: Using Docker for Compilation</span></a></h2><p>Using Docker for compilation is the simplest method. You only need to ensure that you have installed <strong>docker</strong> and <strong>docker-compose</strong>. If your host machine only needs to compile libraries and doesn&#39;t need to deploy GPU projects, you may not even need to install a CUDA environment.</p><p>Note that using Docker for compilation has limitations in terms of version selection, such as system and CUDA versions. If you need to modify them, you can check the dockerfile in the project root directory.</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code><span class="line"><span class="token function">docker-compose</span> up build-tensorrt-cuda12-ubuntu22</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h2 id="method-3-directly-download-pre-compiled-libraries" tabindex="-1"><a class="header-anchor" href="#method-3-directly-download-pre-compiled-libraries"><span>Method 3: Directly Download Pre-compiled Libraries</span></a></h2><p>This is the simplest method, but it is for fixed system versions and CUDA versions. You can go directly to the <a href="https://github.com/HyperInspire/InspireFace/releases" target="_blank" rel="noopener noreferrer">Release Page</a> to find pre-compiled CUDA version SDKs.</p><h2 id="how-to-use-the-library" tabindex="-1"><a class="header-anchor" href="#how-to-use-the-library"><span>How to Use the Library</span></a></h2><p>Let&#39;s describe how to use CMake to build a simple example using the CUDA version of InspireFace.</p><p>For convenience, along with the dynamic library, we need to put <strong>toolchain/FindTensorRT.cmake</strong> and the <strong>Megatron_TRT</strong> model into the project:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code><span class="line"><span class="token builtin class-name">.</span></span>
<span class="line">├── CMakeLists.txt</span>
<span class="line">├── FindTensorRT.cmake</span>
<span class="line">├── InspireFace</span>
<span class="line">│   ├── include</span>
<span class="line">│   │   ├── herror.h</span>
<span class="line">│   │   ├── inspireface.h</span>
<span class="line">│   │   └── intypedef.h</span>
<span class="line">│   └── lib</span>
<span class="line">│       └── libInspireFace.so</span>
<span class="line">├── main.cpp</span>
<span class="line">└── models</span>
<span class="line">    └── Megatron_TRT</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>You can refer to this for building <strong>CMakeLists.txt</strong>:</p><div class="language-cmake line-numbers-mode" data-highlighter="prismjs" data-ext="cmake"><pre><code><span class="line"><span class="token keyword">project</span><span class="token punctuation">(</span>InspireFaceCUDA<span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">set</span><span class="token punctuation">(</span><span class="token variable">CMAKE_CXX_STANDARD</span> <span class="token number">14</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># Add the current directory to the CMake module path</span></span>
<span class="line"><span class="token keyword">list</span><span class="token punctuation">(</span>APPEND <span class="token variable">CMAKE_MODULE_PATH</span> <span class="token punctuation">\${</span><span class="token variable">CMAKE_CURRENT_SOURCE_DIR</span><span class="token punctuation">}</span>/<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># Find TensorRT</span></span>
<span class="line"><span class="token keyword">include</span><span class="token punctuation">(</span>FindTensorRT<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># Link InspireFace </span></span>
<span class="line"><span class="token keyword">link_directories</span><span class="token punctuation">(</span><span class="token punctuation">\${</span><span class="token variable">CMAKE_CURRENT_SOURCE_DIR</span><span class="token punctuation">}</span>/InspireFace/lib<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># Include InspireFace headers</span></span>
<span class="line"><span class="token keyword">include_directories</span><span class="token punctuation">(</span><span class="token punctuation">\${</span><span class="token variable">CMAKE_CURRENT_SOURCE_DIR</span><span class="token punctuation">}</span>/InspireFace/include<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token keyword">add_executable</span><span class="token punctuation">(</span>demo main.cpp<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token keyword">target_link_libraries</span><span class="token punctuation">(</span>demo <span class="token punctuation">\${</span>TENSORRT_LIBRARIES<span class="token punctuation">}</span> InspireFace<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Besides building, the InspireFace CAPI provides some CUDA-related interfaces, mainly utility functions for printing GPU information, checking availability, etc. Other facial algorithm functionality interfaces <strong>are consistent with the general version</strong>, so we won&#39;t elaborate on how to use the API.</p><div class="language-c line-numbers-mode" data-highlighter="prismjs" data-ext="c"><pre><code><span class="line"><span class="token comment">/**</span>
<span class="line"> * @brief Set the Apple CoreML inference mode, must be called before HFCreateInspireFaceSession.</span>
<span class="line"> * @param mode The inference mode to be set.</span>
<span class="line"> * @return HResult indicating the success or failure of the operation.</span>
<span class="line"> * */</span></span>
<span class="line">HYPER_CAPI_EXPORT <span class="token keyword">extern</span> HResult <span class="token function">HFSetAppleCoreMLInferenceMode</span><span class="token punctuation">(</span>HFAppleCoreMLInferenceMode mode<span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line"></span>
<span class="line"><span class="token comment">/**</span>
<span class="line"> * @brief Set the CUDA device id, must be called before HFCreateInspireFaceSession.</span>
<span class="line"> * @param device_id The device id to be set.</span>
<span class="line"> * @return HResult indicating the success or failure of the operation.</span>
<span class="line"> * */</span></span>
<span class="line">HYPER_CAPI_EXPORT <span class="token keyword">extern</span> HResult <span class="token function">HFSetCudaDeviceId</span><span class="token punctuation">(</span><span class="token class-name">int32_t</span> device_id<span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line"></span>
<span class="line"><span class="token comment">/**</span>
<span class="line"> * @brief Get the CUDA device id, must be called after HFCreateInspireFaceSession.</span>
<span class="line"> * @param device_id Pointer to the device id to be returned.</span>
<span class="line"> * @return HResult indicating the success or failure of the operation.</span>
<span class="line"> * */</span></span>
<span class="line">HYPER_CAPI_EXPORT <span class="token keyword">extern</span> HResult <span class="token function">HFGetCudaDeviceId</span><span class="token punctuation">(</span><span class="token class-name">int32_t</span> <span class="token operator">*</span>device_id<span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line"></span>
<span class="line"><span class="token comment">/**</span>
<span class="line"> * @brief Print the CUDA device information.</span>
<span class="line"> * @return HResult indicating the success or failure of the operation.</span>
<span class="line"> * */</span></span>
<span class="line">HYPER_CAPI_EXPORT <span class="token keyword">extern</span> HResult <span class="token function">HFPrintCudaDeviceInfo</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line"></span>
<span class="line"><span class="token comment">/**</span>
<span class="line"> * @brief Get the number of CUDA devices.</span>
<span class="line"> * @param num_devices Pointer to the number of CUDA devices to be returned.</span>
<span class="line"> * @return HResult indicating the success or failure of the operation.</span>
<span class="line"> * */</span></span>
<span class="line">HYPER_CAPI_EXPORT <span class="token keyword">extern</span> HResult <span class="token function">HFGetNumCudaDevices</span><span class="token punctuation">(</span><span class="token class-name">int32_t</span> <span class="token operator">*</span>num_devices<span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line"></span>
<span class="line"><span class="token comment">/**</span>
<span class="line"> * @brief Check if the CUDA device is supported.</span>
<span class="line"> * @param support The support flag to be checked.</span>
<span class="line"> * @return HResult indicating the success or failure of the operation.</span>
<span class="line"> * */</span></span>
<span class="line">HYPER_CAPI_EXPORT <span class="token keyword">extern</span> HResult <span class="token function">HFCheckCudaDeviceSupport</span><span class="token punctuation">(</span><span class="token class-name">int32_t</span> <span class="token operator">*</span>is_support<span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,48)]))}const o=s(l,[["render",t]]),r=JSON.parse('{"path":"/using-with/cuda.html","title":"Using InspireFace in CUDA","lang":"en-US","frontmatter":{},"headers":[{"level":2,"title":"Checking CUDA Installation","slug":"checking-cuda-installation","link":"#checking-cuda-installation","children":[]},{"level":2,"title":"Method 1: Local Compilation","slug":"method-1-local-compilation","link":"#method-1-local-compilation","children":[{"level":3,"title":"Installing TensorRT","slug":"installing-tensorrt","link":"#installing-tensorrt","children":[]},{"level":3,"title":"Executing Compilation","slug":"executing-compilation","link":"#executing-compilation","children":[]},{"level":3,"title":"Compilation Completion","slug":"compilation-completion","link":"#compilation-completion","children":[]}]},{"level":2,"title":"Method 2: Using Docker for Compilation","slug":"method-2-using-docker-for-compilation","link":"#method-2-using-docker-for-compilation","children":[]},{"level":2,"title":"Method 3: Directly Download Pre-compiled Libraries","slug":"method-3-directly-download-pre-compiled-libraries","link":"#method-3-directly-download-pre-compiled-libraries","children":[]},{"level":2,"title":"How to Use the Library","slug":"how-to-use-the-library","link":"#how-to-use-the-library","children":[]}],"git":{"updatedTime":1744296882000,"contributors":[{"name":"tunm","username":"tunm","email":"tunmxy@163.com","commits":1,"url":"https://github.com/tunm"},{"name":"Jingyu","username":"Jingyu","email":"tunmxy@163.com","commits":1,"url":"https://github.com/Jingyu"}],"changelog":[{"hash":"4e1ab5daddba6a91f0c9e130a9fd26ab3d56aad4","time":1744296882000,"email":"tunmxy@163.com","author":"Jingyu","message":"Update"},{"hash":"36283da830e3e7efee25add2666d4a49dae22fa8","time":1744178190000,"email":"tunmxy@163.com","author":"tunm","message":"Update"}]},"filePathRelative":"using-with/cuda.md"}');export{o as comp,r as data};
